{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Getting a dataset\n",
    "For demonstration purposes, we will use the FashionMNIST dataset, which is easily accessible through Torchvision. This dataset contains greyscale images of 10 different types of clothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "# Setup the training data\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\", # directory that contains the dataset\n",
    "    train=True, # Traing or test split\n",
    "    download=True, # Should the data be downloaded or not\n",
    "    transform=ToTensor() # Transforms that will be applied to the data. We need to use ToTensor transform to convert the images data into a tensor\n",
    ")\n",
    "\n",
    "# Setup the testing data\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a sample image and label from the training data\n",
    "# Image is a tensor that contains our image data, while label is an int that tells us what category this image belongs to (bag, coat, dress, etc).\n",
    "image, label = training_data[0]\n",
    "\n",
    "print(f\"shape of the image tensor: {image.shape}\")\n",
    "print(f\"Label of the image: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Input and output shapes of a computer vision model\n",
    "As you can see above the shape of the image tensor is [1, 28, 28] where [color_channels=1, height=28, width=28]. Input and output shapes of a model will depend on your task and model you are using. Generally images tensor shapes are **CHW** (Color Channels, Height, Width) or **HWC** (Height, Width, Color Channels). Later we will see shapes like **NCHW** or **NHWC** where **N** is the batch size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"training images:{len(training_data.data)}\") \n",
    "print(f\"training targets:{len(training_data.targets)}\")\n",
    "\n",
    "print(f\"test images:{len(test_data.data)}\")\n",
    "print(f\"test targets:{len(test_data.data)}\")\n",
    "\n",
    "print(f\"class names {training_data.classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "labels_map = training_data.classes  # Map of label indices to label names\n",
    "\n",
    "# Create a figure with a specified size\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3  # Define the number of columns and rows for the grid\n",
    "\n",
    "# Loop through to create a grid of images\n",
    "for i in range(1, cols * rows + 1):\n",
    "    # Get a random sample index\n",
    "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
    "    # Get the image and label at the random index\n",
    "    img, label = training_data[sample_idx]\n",
    "    # Add a subplot in the correct grid position\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    # Set the title of the subplot to the label name\n",
    "    plt.title(labels_map[label])\n",
    "    # Turn off the axis\n",
    "    plt.axis(\"off\")\n",
    "    # Display the image, removing singleton dimensions and setting the color map to gray\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "\n",
    "# Show the figure with all the subplots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloaders\n",
    "Now that we have a dataset next step is to create a **Dataloader**. \n",
    "What is a dataloader?\n",
    "A DataLoader is a PyTorch class that provides an efficient and flexible way to load data in batches for training and evaluating models.\n",
    "\n",
    "Key Functions and Benefits of DataLoader:\n",
    "\n",
    "    Batch Processing:\n",
    "        DataLoader allows you to easily divide your dataset into smaller batches. This is important because processing the entire dataset at once can be computationally expensive and memory-intensive. By splitting the data into batches, you can fit the data processing within the memory limits of your hardware.\n",
    "\n",
    "    Shuffling:\n",
    "        It supports shuffling of the dataset, which helps in training models more effectively by ensuring that the model does not learn the order of the data. This helps to reduce overfitting and improves generalization.\n",
    "\n",
    "    Parallel Data Loading:\n",
    "        DataLoader can load data in parallel using multiple worker threads. This parallelism helps in speeding up the data loading process, especially when working with large datasets.\n",
    "\n",
    "    Handling Data Transformations:\n",
    "        It can handle various data transformations (like normalization, augmentation, etc.) on-the-fly. This is particularly useful for data preprocessing steps that need to be applied to each batch.\n",
    "\n",
    "    Custom Batching:\n",
    "        DataLoader allows you to define custom collate functions, which can be used to combine different samples into a single batch in a custom manner. This is useful for tasks that require specific ways of handling batches (e.g., varying image sizes, sequences of different lengths).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# the batch size we will use\n",
    "batch_size = 16\n",
    "\n",
    "# Create dataloaders for training and testing.\n",
    "train_dataloader = DataLoader(dataset=training_data, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "test_dataloader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- dataset: The dataset from which to load the data.\n",
    "- batch_size: The number of samples per batch to load.\n",
    "- shuffle: Whether to shuffle the data at every epoch.\n",
    "- num_workers: How many subprocesses to use for data loading. 0 means that the data will be loaded in the main process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# get the images and the labels from the dataloader.\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "\n",
    "# Initialize a figure for plotting\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "\n",
    "# Calculate the number of rows and columns for the grid\n",
    "cols = int(math.sqrt(batch_size))\n",
    "rows = math.ceil(batch_size / cols)\n",
    "\n",
    "# Plot each image in the batch\n",
    "for i in range(batch_size):\n",
    "    figure.add_subplot(rows, cols, i + 1)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(train_features[i].squeeze(), cmap=\"gray\")\n",
    "\n",
    "# Show the figure\n",
    "plt.show()\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
