{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "from sklearn.metrics import classification_report # mamba install scikit-learn or pip3 install scikit-learn to install.\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Set the device (use GPU if available, otherwise fallback to CPU)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the training data\n",
    "training_data = FashionMNIST(\n",
    "    root=\"data\", \n",
    "    train=True, \n",
    "    download=True, \n",
    "    transform=ToTensor()  # Convert images to tensor format\n",
    ")\n",
    "\n",
    "# Setup the testing data\n",
    "test_dataset = FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()  # Convert images to tensor format\n",
    ")\n",
    "\n",
    "train_size = int(0.8 * len(training_data))  # 80% for training\n",
    "valid_size = len(training_data) - train_size  # 20% for testing\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, valid_dataset = random_split(training_data, [train_size, valid_size], \n",
    "                                           generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "# The batch size we will use\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 12\n",
    "\n",
    "# Create dataloaders for training, validation and test\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "valid_dataloader = DataLoader(dataset=valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_stack1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=20, kernel_size=(5, 5)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)),\n",
    "        )\n",
    "\n",
    "        self.conv_stack2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=20, out_channels=50, kernel_size=(5, 5)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)),\n",
    "        )\n",
    "\n",
    "        self.fc_stack = nn.Sequential(\n",
    "            nn.Linear(in_features=800, out_features=500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=500, out_features=10),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_stack1(x)\n",
    "        x = self.conv_stack2(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc_stack(x)\n",
    "        return x\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.001\n",
    "MOMENTUM = 0.9\n",
    "EPOCHS = 20\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=MOMENTUM)\n",
    "\n",
    "# Create output directory with timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "out_dir = f\"output/mnist_{timestamp}\"\n",
    "os.makedirs(os.path.join(out_dir, \"models\"), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the model\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    train_loss, train_acc = 0, 0\n",
    "    model.train()  # Set model to training mode\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)  # Move data to the selected device\n",
    "\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        loss.backward()  # Compute the gradients\n",
    "        optimizer.step()  # Update the weights\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_acc += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            \n",
    "    return train_loss, train_acc\n",
    "\n",
    "# Function to validate the model\n",
    "def validate(dataloader, model, loss_fn):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    val_loss, val_acc = 0, 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)  # Move data to the selected device\n",
    "            pred = model(X)\n",
    "            val_loss += loss_fn(pred, y).item()  # Accumulate validation loss\n",
    "            val_acc += (pred.argmax(1) == y).type(torch.float).sum().item()  # Count correct predictions\n",
    "\n",
    "    return val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = {\n",
    "    \"train_loss\" : [],\n",
    "    \"train_acc\" : [],\n",
    "    \"val_loss\" : [],\n",
    "    \"val_acc\" : []\n",
    "}\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "   \n",
    "    train_loss, train_acc = train(train_dataloader, model, loss_fn, optimizer)\n",
    "    val_loss, val_acc = validate(valid_dataloader, model, loss_fn)\n",
    "\n",
    "    train_loss, train_acc = train_loss / len(train_dataloader), train_acc / len(train_dataloader.dataset)\n",
    "    val_loss, val_acc = val_loss / len(valid_dataloader), val_acc / len(valid_dataloader.dataset)\n",
    "\n",
    "    results[\"train_loss\"].append(train_loss)\n",
    "    results[\"train_acc\"].append(train_acc)\n",
    "    results[\"val_loss\"].append(val_loss)\n",
    "    results[\"val_acc\"].append(val_acc)\n",
    "\n",
    "    print(f\"Training Error: \\n Accuracy: {(100*train_acc):>0.1f}%, Avg loss: {train_loss:>8f} \\n\")\n",
    "    print(f\"Validation Error: \\n Accuracy: {(100*val_acc):>0.1f}%, Avg loss: {val_loss:>8f} \\n\")\n",
    "   \n",
    "    model_save_path = os.path.join(out_dir, \"models\", f\"model_{timestamp}_{epoch}.pth\")\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "    print(f\"Saved model to {model_save_path}\")\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Plotting loss\n",
    "plt.figure()\n",
    "plt.plot(results[\"train_loss\"], label=\"train_loss\")\n",
    "plt.plot(results[\"val_loss\"], label=\"val_loss\")\n",
    "plt.title(\"Training and Validation Loss on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.savefig(os.path.join(out_dir, \"loss_plot.png\"))\n",
    "\n",
    "# Plotting accuracy\n",
    "plt.figure()\n",
    "plt.plot(results[\"train_acc\"], label=\"train_acc\")\n",
    "plt.plot(results[\"val_acc\"], label=\"val_acc\")\n",
    "plt.title(\"Training and Validation Accuracy on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(os.path.join(out_dir, \"accuracy_plot.png\"))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Precision**: This measures the accuracy of the positive predictions. It is the ratio of true positive (TP) predictions to the total number of positive predictions (both true positive and false positive (FP)). The formula is:\n",
    "$$ \\text{Precision} = \\frac{TP}{TP + FP} $$\n",
    "\n",
    "**Recall**: This measures the ability of the model to identify all relevant instances. It is the ratio of true positive predictions to the total number of actual positives (both true positive and false negative (FN)). The formula is:\n",
    "$$ \\text{Recall} = \\frac{TP}{TP + FN} $$\n",
    "\n",
    "**F1 Score**: This is the harmonic mean of precision and recall, providing a single metric that balances both. It is useful when you need a balance between precision and recall. The formula is:\n",
    "$$ F1 \\text{ Score} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the latest trained weights\n",
    "weights = torch.load(model_save_path)\n",
    "model.load_state_dict(weights)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "\tmodel.eval()\n",
    "\tpreds = []\n",
    "\tfor (x, y) in test_dataloader:\n",
    "\t\tx = x.to(device)\n",
    "\t\tpred = model(x)\n",
    "\t\tpreds.extend(pred.argmax(axis=1).cpu().numpy())\n",
    "\n",
    "# generate a classification report\n",
    "print(classification_report(test_dataset.targets.cpu().numpy(),\n",
    "\tnp.array(preds), target_names=test_dataset.classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class names for FashionMNIST\n",
    "class_names = [\n",
    "    \"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "    \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"\n",
    "]\n",
    "\n",
    "# Function to make predictions and display images\n",
    "def show_predictions(model, dataloader, class_names):\n",
    "    model.eval()\n",
    "    fig, axs = plt.subplots(4, 4, figsize=(12, 12))\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (X, y) in enumerate(dataloader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X).argmax(1)\n",
    "            for j in range(len(X)):\n",
    "                if i * BATCH_SIZE + j >= 16:\n",
    "                    break\n",
    "                img, label, prediction = X[j].cpu().squeeze(), y[j].cpu().item(), pred[j].cpu().item()\n",
    "                axs[i * BATCH_SIZE + j].imshow(img, cmap=\"gray\")\n",
    "                axs[i * BATCH_SIZE + j].set_title(f\"True: {class_names[label]}\\nPred: {class_names[prediction]}\")\n",
    "                axs[i * BATCH_SIZE + j].axis('off')\n",
    "            if i * BATCH_SIZE >= 16:\n",
    "                break\n",
    "    plt.show()\n",
    "\n",
    "# Show predictions\n",
    "show_predictions(model, test_dataloader, class_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
